\section{Summary}
The final T5 architecture was modelled as a Text-to-text architecture that provides a unified approach. Along with the common crawl C4 dataset it achieves state of the art performance on many of the NLP tasks. A very detailed comparative study was performed which compared different types of architecture such as BERT style, Language modelling and encoder-decoder model along with their pre-training objectives where the denoising method with spans was found to produce the best results. On comparing different dataset full C4 dataset without any repetition was selected as the best method. While comparing different mixing methods, no clear best method was distinguishable but it gave a comparable performance. For fine tuning the pre-trainined model obtained above on downstream task, it is beneficial to look at methods such as gradual freezing which significantly reduces the time to train. Finally the scaling strategy significantly increase the performance from the baseline architecture providing state of the art performance. \\\\
Even though it has achieved a high accuracy, the architecture of the T5 model remains relatively same to the transformer architecture. Since we use a English language filter on the dataset it cannot be extended easily to other languages. It also might be required to find a balance between computational cost and performance to increase adoption in practical setting. 